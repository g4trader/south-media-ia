#!/usr/bin/env python3
"""
Script principal para execuÃ§Ã£o de testes do sistema South Media IA
Inclui testes unitÃ¡rios, integraÃ§Ã£o, E2E, performance e relatÃ³rios
"""

import os
import sys
import subprocess
import argparse
import time
from datetime import datetime
from pathlib import Path


class TestRunner:
    """Executor principal de testes"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.test_dir = self.project_root / "tests"
        self.reports_dir = self.project_root / "test_reports"
        self.coverage_dir = self.project_root / "htmlcov"
        
        # Criar diretÃ³rios de relatÃ³rios se nÃ£o existirem
        self.reports_dir.mkdir(exist_ok=True)
        self.coverage_dir.mkdir(exist_ok=True)
    
    def run_command(self, command: list, description: str) -> bool:
        """Executar comando e retornar sucesso/falha"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ {description}")
        print(f"{'='*60}")
        print(f"Comando: {' '.join(command)}")
        print(f"DiretÃ³rio: {self.project_root}")
        print(f"{'='*60}\n")
        
        start_time = time.time()
        
        try:
            result = subprocess.run(
                command,
                cwd=self.project_root,
                check=True,
                capture_output=False,
                text=True
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            print(f"\nâœ… {description} concluÃ­do com sucesso em {duration:.2f}s")
            return True
            
        except subprocess.CalledProcessError as e:
            end_time = time.time()
            duration = end_time - start_time
            
            print(f"\nâŒ {description} falhou apÃ³s {duration:.2f}s")
            print(f"CÃ³digo de saÃ­da: {e.returncode}")
            return False
    
    def run_unit_tests(self) -> bool:
        """Executar testes unitÃ¡rios"""
        command = [
            "python", "-m", "pytest",
            "tests/test_models.py",
            "-v",
            "--tb=short",
            "--cov=src/models",
            "--cov-report=html:htmlcov/models",
            "--cov-report=term-missing"
        ]
        
        return self.run_command(command, "Testes UnitÃ¡rios - Modelos")
    
    def run_integration_tests(self) -> bool:
        """Executar testes de integraÃ§Ã£o"""
        command = [
            "python", "-m", "pytest",
            "tests/test_integration_api.py",
            "-v",
            "--tb=short",
            "--cov=src/routes",
            "--cov-report=html:htmlcov/routes",
            "--cov-report=term-missing"
        ]
        
        return self.run_command(command, "Testes de IntegraÃ§Ã£o - API")
    
    def run_performance_tests(self) -> bool:
        """Executar testes de performance"""
        command = [
            "python", "-m", "pytest",
            "tests/test_performance.py",
            "-v",
            "--tb=short",
            "-m", "performance",
            "--durations=10"
        ]
        
        return self.run_command(command, "Testes de Performance")
    
    def run_selenium_tests(self) -> bool:
        """Executar testes E2E com Selenium"""
        command = [
            "python", "-m", "pytest",
            "tests/test_e2e_selenium.py",
            "-v",
            "--tb=short",
            "-m", "selenium",
            "--headless"
        ]
        
        return self.run_command(command, "Testes E2E - Selenium")
    
    def run_all_tests(self) -> bool:
        """Executar todos os testes"""
        command = [
            "python", "-m", "pytest",
            "tests/",
            "-v",
            "--tb=short",
            "--cov=src",
            "--cov-report=html:htmlcov",
            "--cov-report=term-missing",
            "--cov-report=xml:coverage.xml",
            "--cov-fail-under=80",
            "--durations=10",
            "--junitxml=test_reports/junit.xml"
        ]
        
        return self.run_command(command, "Todos os Testes")
    
    def run_coverage_report(self) -> bool:
        """Gerar relatÃ³rio de cobertura detalhado"""
        command = [
            "python", "-m", "coverage",
            "report",
            "--show-missing",
            "--fail-under=80"
        ]
        
        return self.run_command(command, "RelatÃ³rio de Cobertura")
    
    def run_linting(self) -> bool:
        """Executar verificaÃ§Ã£o de cÃ³digo"""
        commands = [
            (["python", "-m", "black", "--check", "src/"], "VerificaÃ§Ã£o Black (formataÃ§Ã£o)"),
            (["python", "-m", "flake8", "src/"], "VerificaÃ§Ã£o Flake8 (estilo)"),
            (["python", "-m", "isort", "--check-only", "src/"], "VerificaÃ§Ã£o isort (imports)"),
            (["python", "-m", "mypy", "src/"], "VerificaÃ§Ã£o MyPy (tipos)"),
            (["python", "-m", "bandit", "-r", "src/"], "VerificaÃ§Ã£o Bandit (seguranÃ§a)")
        ]
        
        all_passed = True
        for command, description in commands:
            if not self.run_command(command, description):
                all_passed = False
        
        return all_passed
    
    def run_smoke_tests(self) -> bool:
        """Executar testes de fumaÃ§a (crÃ­ticos)"""
        command = [
            "python", "-m", "pytest",
            "tests/",
            "-v",
            "-m", "smoke",
            "--tb=short"
        ]
        
        return self.run_command(command, "Testes de FumaÃ§a")
    
    def run_regression_tests(self) -> bool:
        """Executar testes de regressÃ£o"""
        command = [
            "python", "-m", "pytest",
            "tests/",
            "-v",
            "-m", "regression",
            "--tb=short"
        ]
        
        return self.run_command(command, "Testes de RegressÃ£o")
    
    def generate_test_report(self, results: dict):
        """Gerar relatÃ³rio resumido dos testes"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report_content = f"""
# RelatÃ³rio de Testes - South Media IA
**Data/Hora:** {timestamp}

## Resumo dos Resultados

### Testes UnitÃ¡rios
- Status: {'âœ… PASSOU' if results['unit'] else 'âŒ FALHOU'}

### Testes de IntegraÃ§Ã£o
- Status: {'âœ… PASSOU' if results['integration'] else 'âŒ FALHOU'}

### Testes de Performance
- Status: {'âœ… PASSOU' if results['performance'] else 'âŒ FALHOU'}

### Testes E2E (Selenium)
- Status: {'âœ… PASSOU' if results['selenium'] else 'âŒ FALHOU'}

### VerificaÃ§Ã£o de CÃ³digo
- Status: {'âœ… PASSOU' if results['linting'] else 'âŒ FALHOU'}

### Cobertura de CÃ³digo
- Status: {'âœ… PASSOU' if results['coverage'] else 'âŒ FALHOU'}

## EstatÃ­sticas
- Total de Testes: {sum(results.values())}/{len(results)}
- Taxa de Sucesso: {(sum(results.values()) / len(results)) * 100:.1f}%

## PrÃ³ximos Passos
{'ğŸ‰ Todos os testes passaram! O sistema estÃ¡ pronto para produÃ§Ã£o.' if all(results.values()) else 'âš ï¸ Alguns testes falharam. Verifique os logs e corrija os problemas antes de prosseguir.'}

---
*RelatÃ³rio gerado automaticamente pelo sistema de testes*
"""
        
        # Salvar relatÃ³rio
        report_file = self.reports_dir / f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"\nğŸ“Š RelatÃ³rio salvo em: {report_file}")
        
        # Exibir relatÃ³rio no console
        print(report_content)
    
    def run_test_suite(self, test_type: str = "all"):
        """Executar suite de testes especÃ­fica"""
        results = {}
        
        if test_type == "all" or test_type == "unit":
            results['unit'] = self.run_unit_tests()
        
        if test_type == "all" or test_type == "integration":
            results['integration'] = self.run_integration_tests()
        
        if test_type == "all" or test_type == "performance":
            results['performance'] = self.run_performance_tests()
        
        if test_type == "all" or test_type == "selenium":
            results['selenium'] = self.run_selenium_tests()
        
        if test_type == "all" or test_type == "linting":
            results['linting'] = self.run_linting()
        
        if test_type == "all" or test_type == "coverage":
            results['coverage'] = self.run_coverage_report()
        
        if test_type == "all":
            # Executar todos os testes juntos para cobertura completa
            results['all'] = self.run_all_tests()
        
        # Gerar relatÃ³rio
        if results:
            self.generate_test_report(results)
        
        return results


def main():
    """FunÃ§Ã£o principal"""
    parser = argparse.ArgumentParser(
        description="Executor de testes para South Media IA",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python run_tests.py                    # Executar todos os testes
  python run_tests.py --type unit        # Apenas testes unitÃ¡rios
  python run_tests.py --type integration # Apenas testes de integraÃ§Ã£o
  python run_tests.py --type performance # Apenas testes de performance
  python run_tests.py --type selenium    # Apenas testes E2E
  python run_tests.py --type linting     # Apenas verificaÃ§Ã£o de cÃ³digo
  python run_tests.py --type smoke       # Testes de fumaÃ§a
  python run_tests.py --type regression  # Testes de regressÃ£o
        """
    )
    
    parser.add_argument(
        "--type",
        choices=["all", "unit", "integration", "performance", "selenium", "linting", "coverage", "smoke", "regression"],
        default="all",
        help="Tipo de teste a executar"
    )
    
    parser.add_argument(
        "--parallel",
        action="store_true",
        help="Executar testes em paralelo (quando suportado)"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Modo verboso"
    )
    
    args = parser.parse_args()
    
    # Configurar runner
    runner = TestRunner()
    
    # Executar testes
    print(f"ğŸ§ª Iniciando execuÃ§Ã£o de testes: {args.type.upper()}")
    print(f"ğŸ“ DiretÃ³rio do projeto: {runner.project_root}")
    print(f"ğŸ“ DiretÃ³rio de testes: {runner.test_dir}")
    print(f"ğŸ“Š DiretÃ³rio de relatÃ³rios: {runner.reports_dir}")
    
    try:
        results = runner.run_test_suite(args.type)
        
        # Verificar se todos os testes passaram
        if results and all(results.values()):
            print("\nğŸ‰ SUCCESS: Todos os testes passaram!")
            sys.exit(0)
        else:
            print("\nâŒ FAILURE: Alguns testes falharam!")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ExecuÃ§Ã£o interrompida pelo usuÃ¡rio")
        sys.exit(130)
    except Exception as e:
        print(f"\nğŸ’¥ Erro durante execuÃ§Ã£o dos testes: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

